Q-learning: Q-learning is an off-policy reinforcement learning algorithm that aims to learn the optimal action-value function (Q-function). It uses the Bellman equation to update Q-values and is often used for tasks with deterministic environments. Q-learning is known for its off-policy nature, meaning it learns from data generated by one policy while updating another (target) policy.

SARSA: SARSA is an on-policy reinforcement learning algorithm that also learns the action-value function. It uses the SARSA update rule, which incorporates the action chosen by the current policy. SARSA is particularly suited for environments with stochastic transitions. It learns from data generated by the current policy and updates that same policy.

Similarities:
- Both Q-learning and SARSA are used to learn action-value functions.
- They both use the Bellman equation to update Q-values.

Differences:
- Q-learning is off-policy, while SARSA is on-policy.
- In Q-learning, the target policy is greedy (maximizes Q-values), whereas in SARSA, the target policy follows the current policy.
- Q-learning can be more explorative because it learns from a behavior policy while targeting the optimal policy, whereas SARSA directly learns from the current policy.
- SARSA is often used in environments with stochastic transitions because it updates based on actions chosen by the current policy.
