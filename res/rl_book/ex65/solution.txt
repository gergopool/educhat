TD updates incorporate prior information. Suppose we had a good value estimate for a trajectory $\tau = S_1, S_2, \dots, S_T$, then if we try to estimate the trajectory $\tau = S_0, S_1, S_2, \dots, S_T$ using MC then we need to see multiple episodes of this to get a good estimate of $V(S_0)$, not leveraging the info we already have on $\tau$. TD would use info on $\tau$ to back up the value of $S_0$ and hence converge much quicker. The key differences here are bootstrapping and online learning.