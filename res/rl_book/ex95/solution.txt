In the case of the uniform distribution of updates and $b=1$, the start state is visited roughly once every $|\mathcal{S}|$ updates. When this happens, the action values of the neighbourhood of the start state are updated and their undergo a greater change than when the states that are not in the neighbourhood are the start state are updated. Thus, when the policy is evaluated, the value of the start state changes a lot if the start state has been visited recently, and not so much otherwise (since the change comes from values backed up from states far away from the start state).\\

In the on-policy case, the start state is visited much more often (on average more than once every 10 updates, since $\P{}(\texttt{terminate}) = 0.1$) so it does not exhibit this behaviour. When $b$ is larger there are more connections between states, so the neighbourhood of the start state is larger, so this feature is also reduced.