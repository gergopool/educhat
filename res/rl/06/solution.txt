On-policy algorithms use the same policy for both exploration and learning. This means that the agent collects experience data while following its current policy and updates that policy based on the collected data. Common on-policy algorithms include SARSA and A2C.

Off-policy algorithms, on the other hand, maintain separate policies for exploration (behavior policy) and learning (target policy). They learn from historical data generated by an exploration policy and improve the target policy. Q-learning and DDPG are examples of off-policy algorithms.