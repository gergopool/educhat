Let $G_t$ be returns from an episode generated by $b$. Then
\begin{align*}
    v_\pi(s) &= \E{}[\rho_{t:T-1}G_t | S_t=s]\\
             &= \E{}[\rho_{t:T-1} R_{t+1} + \gamma \rho_{t:T-1}G_{t+1} | S_t=s] \\
             &= \rho_{t:t} \E{}[R_{t+1}|S_t=s] + \gamma \rho_{t:t}\E{}[\rho_{t+1:T-1}G_{t+1} |S_t=s] \\
             &= \rho_{t:t} \left( \E{}[R_{t+1} | S_t=s] + \E{}[\rho_{t+1:T-1}G_t| S_t = s] \right)\\
             &= \rho_{t:t} \left( r(s, A_t) + v_\pi(S_{t+1})\right).
\end{align*}
So the update for off-policy TD(0) (by sampling approximation) is 
\begin{equation}
    V(S_t) \leftarrow V(S_t) + \alpha \left[ \rho_{t:t} R_{t+1} + \rho_{t:t} \gamma V(S_{t+1}) - V(S_t) \right].
\end{equation}